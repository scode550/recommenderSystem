{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":77759,"sourceType":"datasetVersion","datasetId":339}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom torch.nn import CosineSimilarity\nimport dask.dataframe as dd\nimport polars as pl\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nratings=pd.read_csv('/kaggle/input/movielens-20m-dataset/rating.csv')\ndel ratings['timestamp']\ndf=pl.DataFrame(ratings)\nratings1 = ratings[:500000]\nratings2 = ratings[500000:1000000]\nratings3 = ratings[1000000:1500000]\nratings4 = ratings[1500000:2000000]\nratings5 = ratings[2000000:2500000]\nratings6 = ratings[2500000:3000000]\nratings7 = ratings[3000000:3500000]\nratings8 = ratings[3500000:4000000]\nratings9 = ratings[4000000:4500000]\nratings10 = ratings[4500000:5000000]\nratings11 = ratings[5000000:5500000]\nratings12 = ratings[5500000:6000000]\nratings13 = ratings[6000000:6500000]\nratings14 = ratings[6500000:7000000]\nratings15 = ratings[7000000:7500000]\nratings16 = ratings[7500000:8000000]\nratings17 = ratings[8000000:8500000]\nratings18 = ratings[8500000:9000000]\nratings19 = ratings[9000000:9500000]\nratings20 = ratings[9500000:10000000]\nratings21 = ratings[10000000:10500000]\nratings22 = ratings[10500000:11000000]\nratings23 = ratings[11000000:11500000]\nratings24 = ratings[11500000:12000000]\nratings25 = ratings[12000000:12500000]\nratings26 = ratings[12500000:13000000]\nratings27 = ratings[13000000:13500000]\nratings28 = ratings[13500000:14000000]\nratings29 = ratings[14000000:14500000]\nratings30 = ratings[14500000:15000000]\nratings31 = ratings[15000000:15500000]\nratings32 = ratings[15500000:16000000]\nratings33 = ratings[16000000:16500000]\nratings34 = ratings[16500000:17000000]\nratings35 = ratings[17000000:17500000]\nratings36 = ratings[17500000:18000000]\nratings37 = ratings[18000000:18500000]\nratings38 = ratings[18500000:19000000]\nratings39 = ratings[19000000:19500000]\nratings40 = ratings[19500000:20000000]\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-14T20:45:48.405465Z","iopub.execute_input":"2024-04-14T20:45:48.405982Z","iopub.status.idle":"2024-04-14T20:46:12.350161Z","shell.execute_reply.started":"2024-04-14T20:45:48.405945Z","shell.execute_reply":"2024-04-14T20:46:12.348888Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"/kaggle/input/movielens-20m-dataset/rating.csv\n/kaggle/input/movielens-20m-dataset/link.csv\n/kaggle/input/movielens-20m-dataset/genome_tags.csv\n/kaggle/input/movielens-20m-dataset/genome_scores.csv\n/kaggle/input/movielens-20m-dataset/tag.csv\n/kaggle/input/movielens-20m-dataset/movie.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"print(ratings.head())\nprint(\"\\n Dimensions - \",ratings.shape,'\\n')\nratings.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T20:24:37.864736Z","iopub.execute_input":"2024-04-14T20:24:37.865267Z","iopub.status.idle":"2024-04-14T20:24:37.956054Z","shell.execute_reply.started":"2024-04-14T20:24:37.865215Z","shell.execute_reply":"2024-04-14T20:24:37.954779Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"   userId  movieId  rating\n0       1        2     3.5\n1       1       29     3.5\n2       1       32     3.5\n3       1       47     3.5\n4       1       50     3.5\n\n Dimensions -  (20000263, 3) \n\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"userId     0\nmovieId    0\nrating     0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Assuming you have already split your DataFrame into 40 chunks\nchunks = [ratings1, ratings2, ratings3, ratings4, ratings5, ratings6, ratings7, ratings8, ratings9, ratings10,\n          ratings11, ratings12, ratings13, ratings14, ratings15, ratings16, ratings17, ratings18, ratings19, ratings20,\n          ratings21, ratings22, ratings23, ratings24, ratings25, ratings26, ratings27, ratings28, ratings29, ratings30,\n          ratings31, ratings32, ratings33, ratings34, ratings35, ratings36, ratings37, ratings38, ratings39, ratings40]\n\nresults = []  # To store the result of each chunk\n\nfor i, chunk in enumerate(chunks, start=1):\n    ddf = dd.from_pandas(chunk, npartitions=4)\n    pivot_ddf = ddf.categorize(columns='movieId').pivot_table(index='userId', columns='movieId', values='rating').fillna(0)\n    result = pivot_ddf.compute()\n    results.append(result)\n    print(f\"Processed chunk {i}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T21:00:45.406836Z","iopub.execute_input":"2024-04-14T21:00:45.407335Z","iopub.status.idle":"2024-04-14T21:31:01.798486Z","shell.execute_reply.started":"2024-04-14T21:00:45.407294Z","shell.execute_reply":"2024-04-14T21:31:01.797260Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Processed chunk 1\nProcessed chunk 2\nProcessed chunk 3\nProcessed chunk 4\nProcessed chunk 5\nProcessed chunk 6\nProcessed chunk 7\nProcessed chunk 8\nProcessed chunk 9\nProcessed chunk 10\nProcessed chunk 11\nProcessed chunk 12\nProcessed chunk 13\nProcessed chunk 14\nProcessed chunk 15\nProcessed chunk 16\nProcessed chunk 17\nProcessed chunk 18\nProcessed chunk 19\nProcessed chunk 20\nProcessed chunk 21\nProcessed chunk 22\nProcessed chunk 23\nProcessed chunk 24\nProcessed chunk 25\nProcessed chunk 26\nProcessed chunk 27\nProcessed chunk 28\nProcessed chunk 29\nProcessed chunk 30\nProcessed chunk 31\nProcessed chunk 32\nProcessed chunk 33\nProcessed chunk 34\nProcessed chunk 35\nProcessed chunk 36\nProcessed chunk 37\nProcessed chunk 38\nProcessed chunk 39\nProcessed chunk 40\n","output_type":"stream"}]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}