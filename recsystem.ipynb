{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":77759,"sourceType":"datasetVersion","datasetId":339}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom torch.nn import CosineSimilarity\nimport dask.dataframe as dd\nimport polars as pl\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nratings=pd.read_csv('/kaggle/input/movielens-20m-dataset/rating.csv')\ndel ratings['timestamp']\ndf=pl.DataFrame(ratings)\nratings1 = ratings[:500000]\nratings2 = ratings[500000:1000000]\nratings3 = ratings[1000000:1500000]\nratings4 = ratings[1500000:2000000]\nratings5 = ratings[2000000:2500000]\nratings6 = ratings[2500000:3000000]\nratings7 = ratings[3000000:3500000]\nratings8 = ratings[3500000:4000000]\nratings9 = ratings[4000000:4500000]\nratings10 = ratings[4500000:5000000]\nratings11 = ratings[5000000:5500000]\nratings12 = ratings[5500000:6000000]\nratings13 = ratings[6000000:6500000]\nratings14 = ratings[6500000:7000000]\nratings15 = ratings[7000000:7500000]\nratings16 = ratings[7500000:8000000]\nratings17 = ratings[8000000:8500000]\nratings18 = ratings[8500000:9000000]\nratings19 = ratings[9000000:9500000]\nratings20 = ratings[9500000:10000000]\nratings21 = ratings[10000000:10500000]\nratings22 = ratings[10500000:11000000]\nratings23 = ratings[11000000:11500000]\nratings24 = ratings[11500000:12000000]\nratings25 = ratings[12000000:12500000]\nratings26 = ratings[12500000:13000000]\nratings27 = ratings[13000000:13500000]\nratings28 = ratings[13500000:14000000]\nratings29 = ratings[14000000:14500000]\nratings30 = ratings[14500000:15000000]\nratings31 = ratings[15000000:15500000]\nratings32 = ratings[15500000:16000000]\nratings33 = ratings[16000000:16500000]\nratings34 = ratings[16500000:17000000]\nratings35 = ratings[17000000:17500000]\nratings36 = ratings[17500000:18000000]\nratings37 = ratings[18000000:18500000]\nratings38 = ratings[18500000:19000000]\nratings39 = ratings[19000000:19500000]\nratings40 = ratings[19500000:20000000]\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-14T21:56:01.680013Z","iopub.execute_input":"2024-04-14T21:56:01.680636Z","iopub.status.idle":"2024-04-14T21:56:32.446292Z","shell.execute_reply.started":"2024-04-14T21:56:01.680595Z","shell.execute_reply":"2024-04-14T21:56:32.444904Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/movielens-20m-dataset/rating.csv\n/kaggle/input/movielens-20m-dataset/link.csv\n/kaggle/input/movielens-20m-dataset/genome_tags.csv\n/kaggle/input/movielens-20m-dataset/genome_scores.csv\n/kaggle/input/movielens-20m-dataset/tag.csv\n/kaggle/input/movielens-20m-dataset/movie.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"print(ratings.head())\nprint(\"\\n Dimensions - \",ratings.shape,'\\n')\nratings.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T20:24:37.864736Z","iopub.execute_input":"2024-04-14T20:24:37.865267Z","iopub.status.idle":"2024-04-14T20:24:37.956054Z","shell.execute_reply.started":"2024-04-14T20:24:37.865215Z","shell.execute_reply":"2024-04-14T20:24:37.954779Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"   userId  movieId  rating\n0       1        2     3.5\n1       1       29     3.5\n2       1       32     3.5\n3       1       47     3.5\n4       1       50     3.5\n\n Dimensions -  (20000263, 3) \n\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"userId     0\nmovieId    0\nrating     0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Assuming you have already split your DataFrame into 40 chunks\nchunks = [ratings1, ratings2, ratings3, ratings4, ratings5, ratings6, ratings7, ratings8, ratings9, ratings10,\n          ratings11, ratings12, ratings13, ratings14, ratings15, ratings16, ratings17, ratings18, ratings19, ratings20,\n          ratings21, ratings22, ratings23, ratings24, ratings25, ratings26, ratings27, ratings28, ratings29, ratings30,\n          ratings31, ratings32, ratings33, ratings34, ratings35, ratings36, ratings37, ratings38, ratings39, ratings40]\n\nresults = []  # To store the result of each chunk\n\nfor i, chunk in enumerate(chunks, start=1):\n    ddf = dd.from_pandas(chunk, npartitions=4)\n    pivot_ddf = ddf.categorize(columns='movieId').pivot_table(index='userId', columns='movieId', values='rating').fillna(0)\n    result = pivot_ddf.compute()\n    result.to_csv(f'pivoted_data_{i}.csv')\n    print(f\"Processed and saved chunk {i}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T21:57:10.819621Z","iopub.execute_input":"2024-04-14T21:57:10.820132Z","iopub.status.idle":"2024-04-14T23:05:34.870531Z","shell.execute_reply.started":"2024-04-14T21:57:10.820085Z","shell.execute_reply":"2024-04-14T23:05:34.868465Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Processed and saved chunk 1\nProcessed and saved chunk 2\nProcessed and saved chunk 3\nProcessed and saved chunk 4\nProcessed and saved chunk 5\nProcessed and saved chunk 6\nProcessed and saved chunk 7\nProcessed and saved chunk 8\nProcessed and saved chunk 9\nProcessed and saved chunk 10\nProcessed and saved chunk 11\nProcessed and saved chunk 12\nProcessed and saved chunk 13\nProcessed and saved chunk 14\nProcessed and saved chunk 15\nProcessed and saved chunk 16\nProcessed and saved chunk 17\nProcessed and saved chunk 18\nProcessed and saved chunk 19\nProcessed and saved chunk 20\nProcessed and saved chunk 21\nProcessed and saved chunk 22\nProcessed and saved chunk 23\nProcessed and saved chunk 24\nProcessed and saved chunk 25\nProcessed and saved chunk 26\nProcessed and saved chunk 27\nProcessed and saved chunk 28\nProcessed and saved chunk 29\nProcessed and saved chunk 30\nProcessed and saved chunk 31\nProcessed and saved chunk 32\nProcessed and saved chunk 33\nProcessed and saved chunk 34\nProcessed and saved chunk 35\nProcessed and saved chunk 36\nProcessed and saved chunk 37\nProcessed and saved chunk 38\nProcessed and saved chunk 39\nProcessed and saved chunk 40\n","output_type":"stream"}]},{"cell_type":"code","source":"a=pd.read_csv('pivoted_data_1.csv')\na.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:11:04.729608Z","iopub.execute_input":"2024-04-14T23:11:04.730138Z","iopub.status.idle":"2024-04-14T23:11:27.300359Z","shell.execute_reply.started":"2024-04-14T23:11:04.730049Z","shell.execute_reply":"2024-04-14T23:11:27.298170Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   userId    1    2    3    4    5    6    7    8    9  ...  128601  128622  \\\n0       1  0.0  3.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0     0.0   \n1       2  0.0  0.0  4.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0     0.0   \n2       3  4.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0     0.0   \n3       4  0.0  0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  ...     0.0     0.0   \n4       5  0.0  3.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...     0.0     0.0   \n\n   128686  128715  128832  128842  129354  129428  130219  130490  \n0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n\n[5 rows x 12278 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>128601</th>\n      <th>128622</th>\n      <th>128686</th>\n      <th>128715</th>\n      <th>128832</th>\n      <th>128842</th>\n      <th>129354</th>\n      <th>129428</th>\n      <th>130219</th>\n      <th>130490</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.0</td>\n      <td>3.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 12278 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport glob\nimport shutil\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:20:45.034567Z","iopub.execute_input":"2024-04-14T23:20:45.035221Z","iopub.status.idle":"2024-04-14T23:20:45.043208Z","shell.execute_reply.started":"2024-04-14T23:20:45.035179Z","shell.execute_reply":"2024-04-14T23:20:45.041262Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"csv_files = glob.glob('/kaggle/working/*.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:21:19.750840Z","iopub.execute_input":"2024-04-14T23:21:19.751390Z","iopub.status.idle":"2024-04-14T23:21:19.761043Z","shell.execute_reply.started":"2024-04-14T23:21:19.751338Z","shell.execute_reply":"2024-04-14T23:21:19.759254Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"shutil.make_archive('zipped_csv_files', 'zip', '/kaggle/working/')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T23:22:20.054927Z","iopub.execute_input":"2024-04-14T23:22:20.055517Z","iopub.status.idle":"2024-04-14T23:33:21.273932Z","shell.execute_reply.started":"2024-04-14T23:22:20.055476Z","shell.execute_reply":"2024-04-14T23:33:21.271089Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/zipfile.py:1776\u001b[0m, in \u001b[0;36mZipFile.write\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m src, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(zinfo, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m dest:\n\u001b[0;32m-> 1776\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:198\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[43mfdst_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/zipfile.py:1141\u001b[0m, in \u001b[0;36m_ZipWriteFile.write\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[0;32m-> 1141\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nbytes\n","\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:1009\u001b[0m, in \u001b[0;36m_make_zipfile\u001b[0;34m(base_name, base_dir, verbose, dry_run, logger, owner, group, root_dir)\u001b[0m\n\u001b[1;32m   1008\u001b[0m arcname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(arcdirpath, name)\n\u001b[0;32m-> 1009\u001b[0m \u001b[43mzf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marcname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/zipfile.py:1775\u001b[0m, in \u001b[0;36mZipFile.write\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m src, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(zinfo, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m dest:\n\u001b[1;32m   1776\u001b[0m         shutil\u001b[38;5;241m.\u001b[39mcopyfileobj(src, dest, \u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m8\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/zipfile.py:1170\u001b[0m, in \u001b[0;36m_ZipWriteFile.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_size \u001b[38;5;241m>\u001b[39m ZIP64_LIMIT:\n\u001b[0;32m-> 1170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile size unexpectedly exceeded ZIP64 limit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_size \u001b[38;5;241m>\u001b[39m ZIP64_LIMIT:\n","\u001b[0;31mRuntimeError\u001b[0m: File size unexpectedly exceeded ZIP64 limit","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/zipfile.py:1838\u001b[0m, in \u001b[0;36mZipFile.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seekable:\n\u001b[0;32m-> 1838\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_end_record()\n","\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzipped_csv_files\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:1124\u001b[0m, in \u001b[0;36mmake_archive\u001b[0;34m(base_name, format, root_dir, base_dir, verbose, dry_run, owner, group, logger)\u001b[0m\n\u001b[1;32m   1121\u001b[0m             os\u001b[38;5;241m.\u001b[39mchdir(root_dir)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1124\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_cwd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:983\u001b[0m, in \u001b[0;36m_make_zipfile\u001b[0;34m(base_name, base_dir, verbose, dry_run, logger, owner, group, root_dir)\u001b[0m\n\u001b[1;32m    979\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreating \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and adding \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to it\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    980\u001b[0m                 zip_filename, base_dir)\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dry_run:\n\u001b[0;32m--> 983\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(zip_filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    984\u001b[0m                          compression\u001b[38;5;241m=\u001b[39mzipfile\u001b[38;5;241m.\u001b[39mZIP_DEFLATED) \u001b[38;5;28;01mas\u001b[39;00m zf:\n\u001b[1;32m    985\u001b[0m         arcname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(base_dir)\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m root_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/zipfile.py:1312\u001b[0m, in \u001b[0;36mZipFile.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):\n\u001b[0;32m-> 1312\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/zipfile.py:1843\u001b[0m, in \u001b[0;36mZipFile.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1841\u001b[0m fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1843\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fpclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/zipfile.py:1943\u001b[0m, in \u001b[0;36mZipFile._fpclose\u001b[0;34m(self, fp)\u001b[0m\n\u001b[1;32m   1941\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileRefCnt \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileRefCnt \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filePassed:\n\u001b[0;32m-> 1943\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"],"ename":"OSError","evalue":"[Errno 28] No space left on device","output_type":"error"}]},{"cell_type":"code","source":"from mrjob.job import MRJob\nfrom mrjob.step import MRStep\nimport csv\nfrom collections import defaultdict\nfrom mrjob import conf\nconf.HADOOP_HOME = 'D:\\hadoop-3.4.0'\n# Optionally, specify Hadoop command directly\nconf.HADOOP_CMD = 'D:\\hadoop-3.4.0\\\\bin\\hadoop'\n\n\n\nclass PreprocessData(MRJob):\n    def mapper_init(self):\n        self.user_ratings = defaultdict(dict)\n        with open('input_data.csv', 'r') as f:\n            reader = csv.reader(f)\n            header = next(reader)  # Skip the header row\n            for row in reader:\n                userId = row[0]\n                for i, rating_str in enumerate(row[1:], start=1):\n                    if rating_str.strip():  # Check if rating is not empty\n                        movieId = str(i)\n                        rating = float(rating_str.strip())\n                        self.user_ratings[userId][movieId] = rating\n\n    def mapper(self, _, line):\n        pass  # Mapper is not needed since data is already preprocessed\n\nclass CalculateSimilarity(MRJob):\n    def mapper(self, userId, movie_ratings):\n        movie_ratings = dict(movie_ratings)\n        for other_user, other_ratings in self.user_ratings.items():\n            if other_user != userId:\n                shared_movies = set(movie_ratings.keys()) & set(other_ratings.keys())\n                if shared_movies:\n                    intersection_ratings = [(movie_ratings[movie], other_ratings[movie]) for movie in shared_movies]\n                    yield (userId, other_user), intersection_ratings\n\n    def reducer(self, user_pair, intersection_ratings):\n        intersection_ratings = [x for sublist in intersection_ratings for x in sublist]\n        sum_xx = sum(xy[0] ** 2 for xy in intersection_ratings)\n        sum_yy = sum(xy[1] ** 2 for xy in intersection_ratings)\n        sum_xy = sum(xy[0] * xy[1] for xy in intersection_ratings)\n        similarity = sum_xy / ((sum_xx ** 0.5) * (sum_yy ** 0.5))\n        yield user_pair, similarity\n\nclass GenerateRecommendations(MRJob):\n    def mapper(self, _, line):\n        userId, neighbors = line.strip().split('\\t')\n        neighbors = neighbors.split(',')\n        recommendations = {}\n        for neighbor in neighbors:\n            neighborId, similarity = neighbor.split(':')\n            movies = neighbor.split(',')\n            for movie in movies:\n                movieId, neighborRating = movie.split(':')\n                if movieId not in recommendations:\n                    recommendations[movieId] = 0\n                recommendations[movieId] += float(similarity) * float(neighborRating)\n        for movieId, score in recommendations.items():\n            yield userId, (movieId, score)\n\n    def reducer(self, userId, movie_scores):\n        recommendations = sorted(movie_scores, key=lambda x: x[1], reverse=True)[:10]  # Get top 10 recommendations\n        for movieId, score in recommendations:\n            yield userId, movieId, score\n\nif __name__ == '__main__':\n    # Preprocess data\n    job = PreprocessData()\n    with job.make_runner() as runner:\n        runner.run()\n\n    # Calculate user-user similarity\n    job = CalculateSimilarity()\n    with job.make_runner() as runner:\n        runner.run()\n\n    # Collect user-user similarity scores into memory\n    similarity_scores = defaultdict(dict)\n    with open('output_file.txt', 'r') as f:\n        for line in f:\n            user_pair, similarity = line.strip().split('\\t')\n            user1, user2 = user_pair.split(',')\n            similarity_scores[user1][user2] = float(similarity)\n\n    # Generate recommendations\n    job = GenerateRecommendations()\n    job.user_similarities = similarity_scores  # Pass similarity scores to the mapper\n    with job.make_runner() as runner:\n        runner.run()\n","metadata":{},"execution_count":null,"outputs":[]}]}