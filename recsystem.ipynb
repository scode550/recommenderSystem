{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":77759,"sourceType":"datasetVersion","datasetId":339}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom torch.nn import CosineSimilarity\nimport dask.dataframe as dd\nimport polars as pl\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nratings=pd.read_csv('/kaggle/input/movielens-20m-dataset/rating.csv')\ndel ratings['timestamp']\ndf=pl.DataFrame(ratings)\nratings1 = ratings[:500000]\nratings2 = ratings[500000:1000000]\nratings3 = ratings[1000000:1500000]\nratings4 = ratings[1500000:2000000]\nratings5 = ratings[2000000:2500000]\nratings6 = ratings[2500000:3000000]\nratings7 = ratings[3000000:3500000]\nratings8 = ratings[3500000:4000000]\nratings9 = ratings[4000000:4500000]\nratings10 = ratings[4500000:5000000]\nratings11 = ratings[5000000:5500000]\nratings12 = ratings[5500000:6000000]\nratings13 = ratings[6000000:6500000]\nratings14 = ratings[6500000:7000000]\nratings15 = ratings[7000000:7500000]\nratings16 = ratings[7500000:8000000]\nratings17 = ratings[8000000:8500000]\nratings18 = ratings[8500000:9000000]\nratings19 = ratings[9000000:9500000]\nratings20 = ratings[9500000:10000000]\nratings21 = ratings[10000000:10500000]\nratings22 = ratings[10500000:11000000]\nratings23 = ratings[11000000:11500000]\nratings24 = ratings[11500000:12000000]\nratings25 = ratings[12000000:12500000]\nratings26 = ratings[12500000:13000000]\nratings27 = ratings[13000000:13500000]\nratings28 = ratings[13500000:14000000]\nratings29 = ratings[14000000:14500000]\nratings30 = ratings[14500000:15000000]\nratings31 = ratings[15000000:15500000]\nratings32 = ratings[15500000:16000000]\nratings33 = ratings[16000000:16500000]\nratings34 = ratings[16500000:17000000]\nratings35 = ratings[17000000:17500000]\nratings36 = ratings[17500000:18000000]\nratings37 = ratings[18000000:18500000]\nratings38 = ratings[18500000:19000000]\nratings39 = ratings[19000000:19500000]\nratings40 = ratings[19500000:20000000]\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-14T21:56:01.680013Z","iopub.execute_input":"2024-04-14T21:56:01.680636Z","iopub.status.idle":"2024-04-14T21:56:32.446292Z","shell.execute_reply.started":"2024-04-14T21:56:01.680595Z","shell.execute_reply":"2024-04-14T21:56:32.444904Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/movielens-20m-dataset/rating.csv\n/kaggle/input/movielens-20m-dataset/link.csv\n/kaggle/input/movielens-20m-dataset/genome_tags.csv\n/kaggle/input/movielens-20m-dataset/genome_scores.csv\n/kaggle/input/movielens-20m-dataset/tag.csv\n/kaggle/input/movielens-20m-dataset/movie.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"print(ratings.head())\nprint(\"\\n Dimensions - \",ratings.shape,'\\n')\nratings.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T20:24:37.864736Z","iopub.execute_input":"2024-04-14T20:24:37.865267Z","iopub.status.idle":"2024-04-14T20:24:37.956054Z","shell.execute_reply.started":"2024-04-14T20:24:37.865215Z","shell.execute_reply":"2024-04-14T20:24:37.954779Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"   userId  movieId  rating\n0       1        2     3.5\n1       1       29     3.5\n2       1       32     3.5\n3       1       47     3.5\n4       1       50     3.5\n\n Dimensions -  (20000263, 3) \n\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"userId     0\nmovieId    0\nrating     0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Assuming you have already split your DataFrame into 40 chunks\nchunks = [ratings1, ratings2, ratings3, ratings4, ratings5, ratings6, ratings7, ratings8, ratings9, ratings10,\n          ratings11, ratings12, ratings13, ratings14, ratings15, ratings16, ratings17, ratings18, ratings19, ratings20,\n          ratings21, ratings22, ratings23, ratings24, ratings25, ratings26, ratings27, ratings28, ratings29, ratings30,\n          ratings31, ratings32, ratings33, ratings34, ratings35, ratings36, ratings37, ratings38, ratings39, ratings40]\n\nresults = []  # To store the result of each chunk\n\nfor i, chunk in enumerate(chunks, start=1):\n    ddf = dd.from_pandas(chunk, npartitions=4)\n    pivot_ddf = ddf.categorize(columns='movieId').pivot_table(index='userId', columns='movieId', values='rating').fillna(0)\n    result = pivot_ddf.compute()\n    result.to_csv(f'pivoted_data_{i}.csv')\n    print(f\"Processed and saved chunk {i}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T21:57:10.819621Z","iopub.execute_input":"2024-04-14T21:57:10.820132Z","iopub.status.idle":"2024-04-14T23:05:34.870531Z","shell.execute_reply.started":"2024-04-14T21:57:10.820085Z","shell.execute_reply":"2024-04-14T23:05:34.868465Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Processed and saved chunk 1\nProcessed and saved chunk 2\nProcessed and saved chunk 3\nProcessed and saved chunk 4\nProcessed and saved chunk 5\nProcessed and saved chunk 6\nProcessed and saved chunk 7\nProcessed and saved chunk 8\nProcessed and saved chunk 9\nProcessed and saved chunk 10\nProcessed and saved chunk 11\nProcessed and saved chunk 12\nProcessed and saved chunk 13\nProcessed and saved chunk 14\nProcessed and saved chunk 15\nProcessed and saved chunk 16\nProcessed and saved chunk 17\nProcessed and saved chunk 18\nProcessed and saved chunk 19\nProcessed and saved chunk 20\nProcessed and saved chunk 21\nProcessed and saved chunk 22\nProcessed and saved chunk 23\nProcessed and saved chunk 24\nProcessed and saved chunk 25\nProcessed and saved chunk 26\nProcessed and saved chunk 27\nProcessed and saved chunk 28\nProcessed and saved chunk 29\nProcessed and saved chunk 30\nProcessed and saved chunk 31\nProcessed and saved chunk 32\nProcessed and saved chunk 33\nProcessed and saved chunk 34\nProcessed and saved chunk 35\nProcessed and saved chunk 36\nProcessed and saved chunk 37\nProcessed and saved chunk 38\nProcessed and saved chunk 39\nProcessed and saved chunk 40\n","output_type":"stream"}]}]}